{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import networkx as nx\n",
    "import requests\n",
    "URL = 'https://www.deu.edu.tr/'\n",
    "HEADERS = {\n",
    "    # changing the user agent to avoid 403 error\n",
    "    'User-Agent': 'My User Agent 1.0',\n",
    "}\n",
    "\n",
    "crawledURLs = []\n",
    "GRAPH = nx.DiGraph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageContent(url):\n",
    "    page = requests.get(url, headers=HEADERS)\n",
    "    return BeautifulSoup(page.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterLinks(links):\n",
    "    filteredLinks = []\n",
    "    blockedFileExtensions = [\n",
    "        'pdf',\n",
    "        'jpg',\n",
    "        'jpeg',\n",
    "        'png',\n",
    "        'gif',\n",
    "        'doc',\n",
    "        'docx',\n",
    "        'xls',\n",
    "        'xlsx',\n",
    "        'ppt',\n",
    "        'pptx',\n",
    "        'zip',\n",
    "        'rar',\n",
    "        'tar',\n",
    "        'gz',\n",
    "        'exe',\n",
    "        'mp4',\n",
    "        'login.php'\n",
    "    ]\n",
    "    for link in links:\n",
    "        parsedLink = link.get('href')\n",
    "        # if link is external link and not in links list add it to links list and it is not a file\n",
    "        if parsedLink and 'deu.edu.tr' in parsedLink and parsedLink not in crawledURLs and parsedLink not in filteredLinks and not parsedLink.endswith(tuple(blockedFileExtensions)):\n",
    "            filteredLinks.append(parsedLink)\n",
    "    return filteredLinks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinks(content):\n",
    "    if not content:\n",
    "        return []\n",
    "\n",
    "    filteredLinks = filterLinks(content.find_all('a'))\n",
    "    return filteredLinks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWords(content):\n",
    "    words = []\n",
    "    # Extract the text\n",
    "    text = content.get_text()\n",
    "\n",
    "    # Split the text into tokens\n",
    "    tokens = text.split()\n",
    "    for word in tokens:\n",
    "        # append the word and its frequency\n",
    "        words.append(word)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "wordsPerUrl = []\n",
    "wordList = []\n",
    "\n",
    "\n",
    "def crawl(url, depth):\n",
    "    if depth == 0:\n",
    "        return\n",
    "    content = getPageContent(url)\n",
    "    crawledURLs.append(url)\n",
    "    links = getLinks(content)\n",
    "    words = getWords(content)\n",
    "\n",
    "    # Add the edges to the graph\n",
    "    for link in links:\n",
    "        GRAPH.add_edge(url, link)\n",
    "\n",
    "    wordCount = Counter(words)\n",
    "\n",
    "    wordsPerUrl.append({\n",
    "        'url': url,\n",
    "        'words': wordCount\n",
    "    })\n",
    "\n",
    "    wordList.extend(words)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(crawl, link, depth - 1) for link in links]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f'An error occurred: {str(e)} {url}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: HTTPSConnectionPool(host='iramer.deu.edu.tr', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001D3244F74C0>, 'Connection to iramer.deu.edu.tr timed out. (connect timeout=None)')) https://www.deu.edu.tr/\n"
     ]
    }
   ],
   "source": [
    "crawl(URL, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def printDataAsTable(wordsPerUrl):\n",
    "    HEADERS = ['Word', 'Frequency', 'Page Count',]\n",
    "    table = []\n",
    "\n",
    "    # count every word and its url\n",
    "    count = Counter(wordList).most_common(250)\n",
    "\n",
    "    # how many pages that word is in\n",
    "    for word, freq in count:\n",
    "        page_count = 0\n",
    "        for url in wordsPerUrl:\n",
    "            if word in url['words']:\n",
    "                page_count += 1\n",
    "        table.append([word, freq, page_count])\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.table(cellText=table, colLabels=HEADERS,\n",
    "              cellLoc='center', loc='center')\n",
    "    plt.savefig('frequencyTable.jpg', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "printDataAsTable(wordsPerUrl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "def generateWordCloud():\n",
    "    count = Counter(wordList)\n",
    "    wordcloud = WordCloud(width=1920, height=1080,\n",
    "                          background_color='white').generate_from_frequencies(count)\n",
    "\n",
    "    wordcloud.to_image().save('wordcloud.jpg', 'JPEG')\n",
    "\n",
    "\n",
    "generateWordCloud()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDegreeCentrality():\n",
    "    degreeCentrality = nx.degree_centrality(GRAPH)\n",
    "    sortedDegreeCentrality = sorted(\n",
    "        degreeCentrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sortedDegreeCentrality\n",
    "\n",
    "\n",
    "def calculateClosenessCentrality():\n",
    "    closenessCentrality = nx.closeness_centrality(GRAPH)\n",
    "    sortedClosenessCentrality = sorted(\n",
    "        closenessCentrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sortedClosenessCentrality\n",
    "\n",
    "\n",
    "def calculateBetweennessCentrality():\n",
    "    betweennessCentrality = nx.betweenness_centrality(GRAPH)\n",
    "    sortedBetweennessCentrality = sorted(\n",
    "        betweennessCentrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sortedBetweennessCentrality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "degreeCentrality = calculateDegreeCentrality()\n",
    "closenessCentrality = calculateClosenessCentrality()\n",
    "betweennessCentrality = calculateBetweennessCentrality()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportCentralityAsTable(centralityData, filename):\n",
    "    HEADERS = ['URL', 'Centrality']\n",
    "    table = []\n",
    "    # limit to 150\n",
    "    centrality = centralityData[:150]\n",
    "    for url, value in centrality:\n",
    "        table.append([url, value])\n",
    "\n",
    "    plt.axis('off')\n",
    "    table = plt.table(cellText=table, colLabels=HEADERS,\n",
    "                      cellLoc='left', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(8)\n",
    "    table.scale(1, 1.5)  # Adjust the scale to fit the content\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "exportCentralityAsTable(degreeCentrality, 'degreeCentralityTable.jpg')\n",
    "exportCentralityAsTable(closenessCentrality, 'closenessCentralityTable.jpg')\n",
    "exportCentralityAsTable(betweennessCentrality,\n",
    "                        'betweennessCentralityTable.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportCentralityAsGraph(centralityData, filename):\n",
    "    centrality = centralityData[:50]\n",
    "\n",
    "    subgraph_closeness = GRAPH.subgraph([url for url, value in centrality])\n",
    "    pos = nx.spring_layout(subgraph_closeness)\n",
    "    plt.figure()\n",
    "    nx.draw_networkx_nodes(subgraph_closeness, pos, node_size=300)\n",
    "    nx.draw_networkx_edges(subgraph_closeness, pos, alpha=0.1)\n",
    "    nx.draw_networkx_labels(subgraph_closeness, pos, font_size=8)\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "exportCentralityAsGraph(closenessCentrality, 'closenessCentralityGraph.jpg')\n",
    "exportCentralityAsGraph(degreeCentrality, 'degreeCentralityGraph.jpg')\n",
    "exportCentralityAsGraph(betweennessCentrality,\n",
    "                        'betweennessCentralityGraph.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
